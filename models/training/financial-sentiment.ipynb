{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9e707a5",
   "metadata": {},
   "source": [
    "Due to memory issues with the GPU on my laptop, Google Colab was used to train this model. You can use Colab to use a T4 GPU free of charge if you want to run this notebook there. The exact same code was run and can be viewed here:  \n",
    "https://colab.research.google.com/drive/1o0dudLqBLVJ4Eq0zEIlBCL5zWu9nP_bp#scrollTo=su3veNmRFwbn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b11f87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "try:\n",
    "    import transformers\n",
    "except ModuleNotFoundError:\n",
    "    !pip3 install transformers datasets\n",
    "finally:\n",
    "    from transformers import (BertForSequenceClassification, \n",
    "                            BertTokenizerFast,\n",
    "                            PreTrainedTokenizerFast,\n",
    "                            DataCollatorWithPadding,\n",
    "                            AdamW,\n",
    "                            get_scheduler\n",
    "                          )\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fe6349",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 3141\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c830cbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modeling globals\n",
    "BERT = 'nlpaueb/sec-bert-base'\n",
    "#BERT = 'bert-base-uncased'\n",
    "BSZ = 16\n",
    "LR = 0.01\n",
    "MOMENTUM = 0.9\n",
    "DAMPENING = 0\n",
    "WD = 0\n",
    "NESTEROV = True\n",
    "EPOCHS = 10\n",
    "WARMUP_STEPS = 250\n",
    "CLIP = 2.5\n",
    "\n",
    "# Other globals\n",
    "WEIGHT_DIR = './weights/financial-sentiment/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7544f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(WEIGHT_DIR):\n",
    "    pathlib.Path(WEIGHT_DIR).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1d2bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "if DEVICE == 'cuda':\n",
    "    print('Using GPU!')\n",
    "else:\n",
    "    print('Using CPU!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72330a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and prep data\n",
    "data = datasets.load_dataset(\n",
    "    \"JanosAudran/financial-reports-sec\",\n",
    "    \"small_lite\",\n",
    "    split=\"train\"\n",
    ")\n",
    "\n",
    "# Downsample to train=20k and test=2k\n",
    "data = data.train_test_split(train_size=0.5, test_size=0.05, seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568370e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lens = []\n",
    "\n",
    "for row in data['train']['sentence']:\n",
    "    train_lens.append(len(row.split(' ')))\n",
    "\n",
    "MAX_LEN = np.quantile(train_lens, 0.95)\n",
    "print(f\"95% of sentences less than {MAX_LEN} words\")\n",
    "MAX_LEN = int(2 ** (np.ceil(np.log2(MAX_LEN))))\n",
    "print(f\"Setting to nearest power of 2: {MAX_LEN}\")\n",
    "sns.displot(train_lens)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b6bb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(BERT)\n",
    "\n",
    "def tokenize_fx(item, tokenizer: PreTrainedTokenizerFast=tokenizer):\n",
    "    return tokenizer(\n",
    "        text=item['sentence'],\n",
    "        truncation='longest_first',\n",
    "        padding='max_length',\n",
    "        max_length=MAX_LEN,\n",
    "        is_split_into_words=False,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "def get_label(item, label='1d'):\n",
    "    temp = pd.DataFrame(item['labels']).astype(int)\n",
    "    return temp[[label]].to_dict(orient='series')\n",
    "\n",
    "tokenized_data = data.map(tokenize_fx, batched=True)\n",
    "tokenized_data = tokenized_data.map(get_label, batched=True)\n",
    "\n",
    "tokenized_data = tokenized_data.remove_columns([\n",
    "    'cik','sentence','section','filingDate',\n",
    "    'docID','sentenceID','sentenceCount','labels'\n",
    "])\n",
    "tokenized_data = tokenized_data.rename_column('1d', 'labels')\n",
    "tokenized_data.set_format('torch', device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2ea335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check imbalance\n",
    "prop_pos_train = tokenized_data['train']['labels'].float().mean() \n",
    "prop_pos_valid = tokenized_data['test']['labels'].float().mean()\n",
    "\n",
    "if abs(0.5-prop_pos_train) > 0.1:\n",
    "    print(f\"Too much imbalance in train set {prop_pos_train,item()}\")\n",
    "else:\n",
    "    print(f\"Prop train: {prop_pos_train.item()}\")\n",
    "if abs(0.5-prop_pos_valid) > 0.1:\n",
    "    print(f\"Too much imbalance in valid set {prop_pos_valid.item()}\")\n",
    "else:\n",
    "    print(f\"Prop valid: {prop_pos_valid.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7feb67f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    tokenized_data['train'],\n",
    "    shuffle=True,\n",
    "    batch_size=BSZ\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    tokenized_data['test'],\n",
    "    batch_size=BSZ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372673d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate loaders have properly shaped batches\n",
    "exp_shapes = {\n",
    "    'input_ids': torch.Size([BSZ, MAX_LEN]),\n",
    "    'token_type_ids': torch.Size([BSZ, MAX_LEN]),\n",
    "    'attention_mask': torch.Size([BSZ, MAX_LEN]),\n",
    "    'labels': torch.Size([BSZ])\n",
    "}\n",
    "\n",
    "for nm, loader in zip(['train','val'],[train_loader, val_loader]):\n",
    "    print(f\"Checking {nm} loader...\")\n",
    "    for batch in loader:\n",
    "        break\n",
    "    for k,v in batch.items():\n",
    "        act_shape = torch.tensor(v.shape)\n",
    "        exp_shape = torch.tensor(exp_shapes[k])\n",
    "        assert torch.equal(\n",
    "            act_shape,\n",
    "            exp_shape\n",
    "        ), f'\\tSize mismatch for {k}! Got {act_shape}, expected {exp_shape}'\n",
    "    print(\"\\tAll shapes correct!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd6c8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model loading and other prep\n",
    "NUM_LABELS = 2\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    BERT,\n",
    "    num_labels=NUM_LABELS,\n",
    "    problem_type='single_label_classification'\n",
    ")\n",
    "\n",
    "t_batch = {}\n",
    "for k,v in batch.items():\n",
    "    t_batch.update({k: v.to('cpu')})\n",
    "\n",
    "# Check the shape of the output\n",
    "out = model(**t_batch)\n",
    "\n",
    "out_shape = torch.tensor(out.logits.shape)\n",
    "assert torch.equal(\n",
    "    out_shape,\n",
    "    torch.tensor([BSZ, NUM_LABELS])\n",
    "), f'Output shape incorrect! Got {out_shape}, expected [{BSZ},{NUM_LABELS}]'\n",
    "print(\"Output shape good!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe966c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_training_steps = EPOCHS * len(train_loader)\n",
    "optimizer = optim.SGD(\n",
    "    model.parameters(),\n",
    "    lr=LR,\n",
    "    momentum=MOMENTUM,\n",
    "    dampening=DAMPENING,\n",
    "    weight_decay=WD,\n",
    "    nesterov=NESTEROV\n",
    "    )\n",
    "scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=WARMUP_STEPS,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "print(f\"Num Training Steps: {num_training_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd575245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear cuda memory\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506b7672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_scores = []\n",
    "best_loss = np.inf\n",
    "\n",
    "pbar = tqdm(range(num_training_steps+EPOCHS*len(val_loader)))\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.to(DEVICE)\n",
    "    # Training portion\n",
    "    model.train()\n",
    "    pbar.set_description(f\"Epoch {epoch+1}: Train\")\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        out = model(**batch)\n",
    "        loss = out.loss\n",
    "        loss.backward()\n",
    "        train_losses.append(loss.cpu())\n",
    "        \n",
    "        nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        pbar.update(1)\n",
    "        \n",
    "    # Validate portion\n",
    "    model.eval()\n",
    "    pbar.set_description(f\"Epoch {epoch+1}: Validate\")\n",
    "    epoch_val_preds = []\n",
    "    epoch_val_true = []\n",
    "    epoch_val_loss = 0\n",
    "    for batch in val_loader:\n",
    "        with torch.no_grad():\n",
    "            out = model(**batch)\n",
    "        logits = out.logits.cpu()\n",
    "        preds = torch.argmax(logits, dim=-1).int().numpy().tolist()\n",
    "        labels = batch['labels'].cpu().numpy().tolist()\n",
    "        \n",
    "        epoch_val_preds += preds\n",
    "        epoch_val_true += labels\n",
    "        epoch_val_loss += out.loss.cpu()\n",
    "        pbar.update(1)\n",
    "    \n",
    "    # Checkpoint model if it improves\n",
    "    if epoch_val_loss < best_loss:\n",
    "        print('Model improved, saving weights!')\n",
    "        model.cpu().save_pretrained(WEIGHT_DIR)\n",
    "        best_loss = epoch_val_loss\n",
    "    \n",
    "    print(\"Val Loss: {:.6f}\".format(epoch_val_loss/len(val_loader)))\n",
    "    print(classification_report(\n",
    "        epoch_val_true,\n",
    "        epoch_val_preds,\n",
    "        zero_division=0\n",
    "    ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
